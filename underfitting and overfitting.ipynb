{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detect Overfitting and Underfitting with Learning Curves\n",
    "For this quiz, we'll be using three models to train the circular dataset below.\n",
    "\n",
    "A Decision Tree model, a Logistic Regression model, and a Support Vector Machine model.\n",
    "\n",
    "One of the models overfits, one underfits, and the other one is just right. First, we'll write some code to draw the learning curves for each model, and finally we'll look at the learning curves to decide which model is which.\n",
    "\n",
    "First, let's remember that the way the curves look for the three models, is as follows:\n",
    "\n",
    "\n",
    "For the first part of the quiz, all you need is to uncomment one of the classifiers, and hit 'Test Run' to see the graph of the Learning Curve. But if you like coding, here are some details. We'll be using the function called learning_curve:\n",
    "\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(estimator, X, y, cv=None, n_jobs=1, train_sizes=np.linspace(.1, 1.0, num_trainings))\n",
    "\n",
    "No need to worry about all the parameters of this function (you can read some more in here, but here we'll explain the main ones:\n",
    "\n",
    "estimator, is the actual classifier we're using for the data, e.g., LogisticRegression() or GradientBoostingClassifier().\n",
    "\n",
    "1. X and y is our data, split into features and labels.\n",
    "\n",
    "2. train_sizes are the sizes of the chunks of data used to draw each point in the curve.\n",
    "\n",
    "3. train_scores are the training scores for the algorithm trained on each chunk of data.\n",
    "\n",
    "4. test_scores are the testing scores for the algorithm trained on each chunk of data.\n",
    "\n",
    "\n",
    "Two very important observations:\n",
    "\n",
    "The training and testing scores come in as a list of 3 values, and this is because the function uses 3-Fold Cross-Validation.\n",
    "Very important: As you can see, we defined our curves with Training and Testing Error, and this function defines them with Training and Testing Score. These are opposite, so the higher the error, the lower the score. Thus, when you see the curve, you need to flip it upside down in your mind, in order to compare it with the curves above.\n",
    "Part 1: Drawing the learning curves\n",
    "In here, we'll be comparing three models:\n",
    "\n",
    "A Logistic Regression model.\n",
    "A Decision Tree model.\n",
    "A Support Vector Machine model with an rbf kernel, and a gamma parameter of 1000 (this is another type of model, don't worry about how it works for now).\n",
    "\n",
    "Uncomment the code for each one, and examine the learning curve that gets drawn. If you're curious about the code used to draw the learning curves, it's on the utils.py tab.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import, read, and split data\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "data = pd.read_csv('t and t.csv')\n",
    "import numpy as np\n",
    "X = np.array(data[['x1', 'x2']])\n",
    "y = np.array(data['y'])\n",
    "\n",
    "# Fix random seed\n",
    "np.random.seed(55)\n",
    "\n",
    "### Imports\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# TODO: Uncomment one of the three classifiers, and hit \"Test Run\"\n",
    "# to see the learning curve. Use these to answer the quiz below.\n",
    "\n",
    "### Logistic Regression\n",
    "#estimator = LogisticRegression()\n",
    "\n",
    "### Decision Tree\n",
    "#estimator = GradientBoostingClassifier()\n",
    "\n",
    "### Support Vector Machine\n",
    "estimator = SVC(kernel='rbf', gamma=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "# It is good to randomize the data before drawing Learning Curves\n",
    "def randomize(X, Y):\n",
    "    permutation = np.random.permutation(Y.shape[0])\n",
    "    X2 = X[permutation,:]\n",
    "    Y2 = Y[permutation]\n",
    "    return X2, Y2\n",
    "\n",
    "X2, y2 = randomize(X, y)\n",
    "\n",
    "def draw_learning_curves(X, y, estimator, num_trainings):\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X2, y2, cv=None, n_jobs=1, train_sizes=np.linspace(.1, 1.0, num_trainings))\n",
    "\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.grid()\n",
    "\n",
    "    plt.title(\"Learning Curves\")\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "\n",
    "    plt.plot(train_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(test_scores_mean, 'o-', color=\"y\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7e4a145a50e120770814ae6c9986681bab1e6abeb6e6576d2aba2c6a03df04d2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
